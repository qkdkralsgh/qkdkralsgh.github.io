---
layout: single
title: "[논문리뷰] ResNet"
categories: Deep-learning
tag: [Deep-learning, DL]
toc: true
author_profile: false
sidebar:
    nav: "docs"
---
안녕하세요! 이번 포스팅 내용은 ResNet 논문리뷰입니다.


# ResNet

### Intro

- 이전의 연구들로 모델의 layer가 너무 깊어질수록 오히려 성능이 떨어지는 현상이 생김을 밝혀냈다. 이는 **gradient vanishing/exploding 문제** 때문에 학습이 잘 이루어지지 않기 때문이다.
- **gradient vanishing** 이란 layer가 깊어질수록 미분을 점점 많이 하기 때문에 **backpropagation을 해도 앞의 layer일수록 미분값이 작아져 그만큼 output에 영향을 끼치는 weight 정도가 작아지는 것**을 말한다.
- 이는 overfitting과는 다른 문제인데 overfitting은 학습 데이터에 완벽하게 fitting 시킨 탓에 테스트 성능에서는 안좋은 결과를 보임을 뜻하고 위와 같은 문제는 Degradation 문제로 training data에도 학습이 되지 않음을 뜻한다.

따라서 이를 극복하기 위해 ResNet이 고안되었다. ResNet은 skip connection을 이용한 residual learning을 통해 layer가 깊어짐에 따른 gradient vanishing 문제를 해결하였다.

---

### ResNet

기존의  neural net의 학습 목적은 input(x)을 타겟값(y)으로 mapping하는 함수 H(x)를 찾는 것이였다. 

따라서 H(x)-y를 최소화하는 방향으로 학습을 진행한다. 

이때 이미지 classification과 같은 문제의 경우 x에 대한 타겟값 y는 사실 x를 대변하는 것으로 y와 x의 의미가 같게끔 mapping 해야한다. 

즉, 강아지 사진의 pixel값이 input(x)로 주어질 때 이를 2개의 label 중 강아지가 1에 해당한다면 타겟값(y)를 1로 정해서 학습하는 것이 아닌 강아지 사진의 pixel값 (x)로 y를 mapping 해야한다.

따라서 **네트워크의 출력값이 x가 되도록 H(x)-x를 최소화하는 방향으로 학습**을 진행한다.

F(x) = H(x) - x 를 **잔차**라고 하며 이 잔차를 학습하는 것은 **Residual learning** 이라 한다.

<center><img src="/assets/images/ResNet1.png"></center>



이 때 위 그림처럼 네트워크의 output이 x가 되도록 mapping하는 것이 아닌

아래 그림처럼 마지막에 x를 더해서 네트워크의 output은 0이 되도록 mapping해서 최종 output이 x가 되도록 학습한다.

그 이유는 위 그림처럼 단순히 H(x)가 x가 되도록 residual learning으로 학습해도

**결국 gradient vanishing 문제가 해결된 것은 아니다.** 따라서 네트워크는 0이 되도록 학습시키고 마지막에 x를 더해서 H(x)가 x가 되도록 학습하면 미분을 해도 x자체는 미분값 1을 갖기 때문에 각 layer마다

**<center>"최소 gradient로 1은 갖도록 한 것이다"</center>**

<center><img src="/assets/images/ResNet2.png"></center>



따라서 layer가 아무리 깊어져도 최소 gradient로 1이상의 값을 가지므로 gradient vanishing  문제를 해결한 것이다. 

정리하자면 아래와 같다.

---



**1. 이미지에서는 H(x) = x 가 되도록 학습시킨다.**

**2. 네트워크의 output F(x)는 0이 되도록 학습시킨다.**

**3. F(x)+x = H(x) = x 가 되도록 학습시키면 미분해도 F(x)+x 의 미분값은 F'(x)+1로 최소 1 이상이다.**

**4. 모든 layer에서의 gradient가 1+F'(x) 이므로 gradient vanishing 현상을 해결했다.**

---



이렇게 shortcut connection으로 만든 block을 identity block이라고 한다.

그리고 ResNet은 identity block과 convolution block으로 구성되는데 각각은 아래 그림과 같다.

<center><img src="/assets/images/ResNet3.png"></center>

**<center>identity block</center>**





<center><img src="/assets/images/ResNet4.png"></center>

**<center>convolution block</center>**



단순히 **identity block**은 이전까지 설명했듯이 네트워크의 output F(x)에 x를 그대로 더하는 것이고

**convolution block**은 x역시 1x1 convolution 연산을 거친 후 F(x)에 더해주는 것이다.

그리고 ResNet은 이 두가지 block을 아래 그림과 같이 쌓아서 구성한다.



<center><img src="/assets/images/ResNet5.png"></center>

**<center>ResNet structure</center>**



ResNet의 파라미터 구조는 아래 그림과 같다.



<center><img src="/assets/images/ResNet6.png"></center>

**<center>ResNet parameter</center>**



ResNet-101의 경우 각 stage마다 convolution block은 1개씩 존재한다.

따라서 위 그림을 참고하면 identity block은 각 stage에서 2, 3, 22, 2개씩 존재하는 것이다.

---



이상으로 ResNet 논문에 대한 리뷰를 마쳤습니다. 

감사합니다.
